{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "jfTPpJsvdjZx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1733701384829,
     "user": {
      "displayName": "Siyi Wu",
      "userId": "06196688456534608936"
     },
     "user_tz": 360
    },
    "id": "jfTPpJsvdjZx",
    "outputId": "5b384f92-0fc8-4c1d-eae9-0eaa4b2d5896"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96P8EsRKeGJT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 101,
     "status": "ok",
     "timestamp": 1733701385266,
     "user": {
      "displayName": "Siyi Wu",
      "userId": "06196688456534608936"
     },
     "user_tz": 360
    },
    "id": "96P8EsRKeGJT",
    "outputId": "2aa9f03e-cec8-4dab-e260-ecea06b460a3"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/code/WZ'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/code/WZ\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mgetcwd())\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/code/WZ'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"/content/drive/MyDrive/code/WZ\"\n",
    "os.chdir(path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5397fffc",
   "metadata": {
    "executionInfo": {
     "elapsed": 9677,
     "status": "ok",
     "timestamp": 1733701395668,
     "user": {
      "displayName": "Siyi Wu",
      "userId": "06196688456534608936"
     },
     "user_tz": 360
    },
    "id": "5397fffc"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import copy\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "sizef = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a318702",
   "metadata": {
    "id": "9a318702"
   },
   "outputs": [],
   "source": [
    "class Gomoku:\n",
    "    def __init__(self, size=sizef, black_strategy=None, white_strategy=None):\n",
    "        self.size = size  # 棋盘大小\n",
    "        self.board = [['.' for _ in range(size)] for _ in range(size)]  # 初始化棋盘\n",
    "        self.current_player = 'X'  # 当前玩家 ('X' or 'O')\n",
    "        self.black_strategy = black_strategy  # 黑棋策略函数\n",
    "        self.white_strategy = white_strategy  # 白棋策略函数\n",
    "\n",
    "    def display_board(self):\n",
    "        \"\"\"打印棋盘\"\"\"\n",
    "        print(\"   \" + \" \".join(f\"{i:2}\" for i in range(self.size)))\n",
    "        for i, row in enumerate(self.board):\n",
    "            print(f\"{i:2} \" + \" \".join(row))\n",
    "\n",
    "    def is_valid_move(self, x, y):\n",
    "        \"\"\"检查落子是否合法\"\"\"\n",
    "        return 0 <= x < self.size and 0 <= y < self.size and self.board[x][y] == '.'\n",
    "\n",
    "    def make_move(self, x, y):\n",
    "        \"\"\"下棋\"\"\"\n",
    "        if self.is_valid_move(x, y):\n",
    "            self.board[x][y] = self.current_player\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Invalid move by player {self.current_player} at ({x}, {y}).\")\n",
    "            return False\n",
    "\n",
    "    def check_winner(self, x, y):\n",
    "        \"\"\"检查当前玩家是否获胜\"\"\"\n",
    "        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]  # 四个方向：水平、垂直、正斜线、反斜线\n",
    "        for dx, dy in directions:\n",
    "            count = 1\n",
    "            # 检查正方向\n",
    "            for step in range(1, 5):\n",
    "                nx, ny = x + step * dx, y + step * dy\n",
    "                if 0 <= nx < self.size and 0 <= ny < self.size and self.board[nx][ny] == self.current_player:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "            # 检查反方向\n",
    "            for step in range(1, 5):\n",
    "                nx, ny = x - step * dx, y - step * dy\n",
    "                if 0 <= nx < self.size and 0 <= ny < self.size and self.board[nx][ny] == self.current_player:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "            if count >= 5:  # 连续五子\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def switch_player(self):\n",
    "        \"\"\"切换玩家\"\"\"\n",
    "        self.current_player = 'O' if self.current_player == 'X' else 'X'\n",
    "\n",
    "    def play(self):\n",
    "        \"\"\"游戏主循环\"\"\"\n",
    "        print(\"Starting Gomoku!\")\n",
    "        self.display_board()\n",
    "\n",
    "        # 检查是否有 GPU 可用\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Running on device: {device}\")\n",
    "\n",
    "        while True:\n",
    "            if self.current_player == 'X':\n",
    "                if self.black_strategy:\n",
    "                    # 将策略函数运行在指定设备上\n",
    "                    move = self.black_strategy(self.board, 'X', device=device)\n",
    "                else:\n",
    "                    move = self.get_human_move()\n",
    "            else:\n",
    "                if self.white_strategy:\n",
    "                    # 将策略函数运行在指定设备上\n",
    "                    move = self.white_strategy(self.board, 'O', device=device)\n",
    "                else:\n",
    "                    move = self.get_human_move()\n",
    "\n",
    "            if not move or len(move) != 2:\n",
    "                print(f\"Invalid move returned by player {self.current_player}.\")\n",
    "                break\n",
    "\n",
    "            x, y = move\n",
    "            if self.make_move(x, y):\n",
    "                print(f\"Player {self.current_player} places at ({x}, {y})\")\n",
    "                self.display_board()\n",
    "                if self.check_winner(x, y):\n",
    "                    print(f\"Player {self.current_player} wins!\")\n",
    "                    break\n",
    "                self.switch_player()\n",
    "            else:\n",
    "                print(\"Game Over due to invalid move.\")\n",
    "                break\n",
    "\n",
    "        print(\"Game Over!\")\n",
    "\n",
    "    def get_human_move(self):\n",
    "        \"\"\"获取玩家的落子\"\"\"\n",
    "        while True:\n",
    "            try:\n",
    "                move = input(f\"Player {self.current_player}, enter your move (row col): \").strip()\n",
    "                x, y = map(int, move.split())\n",
    "                if self.is_valid_move(x, y):\n",
    "                    return x, y\n",
    "                else:\n",
    "                    print(\"Invalid move. Try again.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter row and column numbers separated by a space.\")\n",
    "\n",
    "\n",
    "def load_strategy(file_name):\n",
    "    \"\"\"从本地文件加载策略\"\"\"\n",
    "    if not os.path.exists(file_name):\n",
    "        raise FileNotFoundError(f\"Strategy file {file_name} not found.\")\n",
    "\n",
    "    module_name = os.path.splitext(os.path.basename(file_name))[0]\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_name)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module.play  # 假设策略文件中定义了一个 play 函数\n",
    "\n",
    "\n",
    "\n",
    "def print_strategy_files(directory):\n",
    "    try:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\"strategy.py\"):\n",
    "                    print(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Invaid: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c801a2",
   "metadata": {
    "id": "90c801a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Gomoku!\n",
      "Choose game mode:\n",
      "1. Human vs AI\n",
      "2. AI vs AI\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1. Human vs AI\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2. AI vs AI\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter 1 or 2: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      7\u001b[0m black_strategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      8\u001b[0m white_strategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1074\u001b[0m     )\n\u001b[0;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to Gomoku!\")\n",
    "print(\"Choose game mode:\")\n",
    "print(\"1. Human vs AI\")\n",
    "print(\"2. AI vs AI\")\n",
    "mode = input(\"Enter 1 or 2: \").strip()\n",
    "\n",
    "black_strategy = None\n",
    "white_strategy = None\n",
    "\n",
    "if mode == \"1\":\n",
    "    print(\"You will play as 'X'.\")\n",
    "    print_strategy_files(os.getcwd())\n",
    "    white_strategy_file = input(\"Enter the AI strategy file for 'O' (e.g., white_strategy.py): \").strip()\n",
    "    white_strategy = load_strategy(white_strategy_file)\n",
    "elif mode == \"2\":\n",
    "    print_strategy_files(os.getcwd())\n",
    "    black_strategy_file = input(\"Enter the AI strategy file for 'X' (e.g., black_strategy.py): \").strip()\n",
    "    white_strategy_file = input(\"Enter the AI strategy file for 'O' (e.g., white_strategy.py): \").strip()\n",
    "    black_strategy = load_strategy(black_strategy_file)\n",
    "    white_strategy = load_strategy(white_strategy_file)\n",
    "else:\n",
    "    print(\"Invalid choice. Exiting.\")\n",
    "\n",
    "game = Gomoku(black_strategy=black_strategy, white_strategy=white_strategy)\n",
    "game.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5873910",
   "metadata": {
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1733701425464,
     "user": {
      "displayName": "Siyi Wu",
      "userId": "06196688456534608936"
     },
     "user_tz": 360
    },
    "id": "c5873910"
   },
   "outputs": [],
   "source": [
    "from alpha0v2_strategy_v2 import AlphaZeroNet\n",
    "from alpha0v2_strategy_v2 import MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "r3lQPHih7ez9",
   "metadata": {
    "executionInfo": {
     "elapsed": 118,
     "status": "ok",
     "timestamp": 1733701428317,
     "user": {
      "displayName": "Siyi Wu",
      "userId": "06196688456534608936"
     },
     "user_tz": 360
    },
    "id": "r3lQPHih7ez9"
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Data Generation via Self-Play\n",
    "# ------------------------------\n",
    "def self_play_game(model, board_size, mcts_simulations=5):\n",
    "    \"\"\"Generate training data via self-play\"\"\"\n",
    "    # 检查是否有 GPU 并将模型移动到 GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 初始化棋盘和 MCTS\n",
    "    board = [['.' for _ in range(board_size)] for _ in range(board_size)]\n",
    "    mcts = MCTS(model, board_size)\n",
    "    current_player = 'X'\n",
    "    last_move = None\n",
    "    game_data = []  # Store (state, policy, value) for training\n",
    "    winner = None\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # Run MCTS to get the move and actions\n",
    "        move, actions = mcts.run(board, current_player, mcts_simulations, last_move=last_move)\n",
    "\n",
    "        # Step 1: 提取所有 (action, Q + u) 对\n",
    "        action_values = [(action, node.get_value(c_puct=5.0)) for action, node in actions.items()]\n",
    "\n",
    "        # Step 2: 计算所有 (Q + u) 的 softmax 概率\n",
    "        values = np.array([value for _, value in action_values])\n",
    "        exp_values = np.exp(values - np.max(values))  # 稳定的 softmax 计算\n",
    "        probabilities = exp_values / np.sum(exp_values)\n",
    "\n",
    "        # Step 3: 生成结果数组\n",
    "        action_probs = [(action, prob) for (action, _), prob in zip(action_values, probabilities)]\n",
    "        \n",
    "        state = mcts.board_to_tensor(board, current_player, last_move).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "        # Make the move\n",
    "        board = mcts.make_move(board, move, current_player)\n",
    "\n",
    "        # Store state and policy\n",
    "#         state = np.zeros((4, board_size, board_size))\n",
    "\n",
    "#         # 当前玩家的棋子位置\n",
    "#         for x in range(board_size):\n",
    "#             for y in range(board_size):\n",
    "#                 if board[x][y] == current_player:\n",
    "#                     state[0][x][y] = 1.0\n",
    "#                 elif board[x][y] != '.':  # 对手的棋子位置\n",
    "#                     state[1][x][y] = 1.0\n",
    "\n",
    "#         # 最近一次落子的位置\n",
    "#         if last_move is not None:\n",
    "#             state[2][last_move[0]][last_move[1]] = 1.0\n",
    "\n",
    "#         # 当前轮到谁下棋\n",
    "#         if current_player == 'O':  # 如果是偶数回合，轮到白棋\n",
    "#             state[3][:, :] = 1.0\n",
    "\n",
    "        # 更新 last_move\n",
    "        last_move = move\n",
    "\n",
    "        # 转换动作概率\n",
    "        action_probs_np = np.zeros((board_size, board_size))\n",
    "        for (x, y), prob in action_probs:\n",
    "            action_probs_np[x, y] = prob\n",
    "\n",
    "        # 检查胜利条件\n",
    "        if mcts.check_winner(board, current_player):\n",
    "            winner = current_player\n",
    "            game_data.append([state, action_probs_np, 1 if winner == 'X' else -1])\n",
    "            break\n",
    "\n",
    "        # 检查平局\n",
    "        if not mcts.get_legal_moves(board):\n",
    "            winner = None  # Draw\n",
    "            game_data.append([state, action_probs_np, 0])\n",
    "            break\n",
    "\n",
    "        # 切换玩家\n",
    "        current_player = 'O' if current_player == 'X' else 'X'\n",
    "\n",
    "    return game_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "qi-k2QvkfIsv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1733614684667,
     "user": {
      "displayName": "Siyi Wu",
      "userId": "06196688456534608936"
     },
     "user_tz": 360
    },
    "id": "qi-k2QvkfIsv",
    "outputId": "a9010201-2a2d-4fc0-dcbd-47b1f849db96"
   },
   "outputs": [],
   "source": [
    "def compute_softmax_probabilities(children):\n",
    "    # Step 1: 提取所有 (action, value_sum) 对\n",
    "    action_values = [(action, node.value_sum) for action, node in children.items()]\n",
    "\n",
    "    # Step 2: 计算所有 value_sum 的 softmax 概率\n",
    "    values = np.array([value_sum for _, value_sum in action_values])\n",
    "    exp_values = np.exp(values - np.max(values))\n",
    "    probabilities = exp_values / np.sum(exp_values)\n",
    "\n",
    "    # Step 3: 生成结果数组\n",
    "    result = [(action, prob) for (action, _), prob in zip(action_values, probabilities)]\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d531a64b",
   "metadata": {
    "executionInfo": {
     "elapsed": 148,
     "status": "ok",
     "timestamp": 1733701435807,
     "user": {
      "displayName": "Siyi Wu",
      "userId": "06196688456534608936"
     },
     "user_tz": 360
    },
    "id": "d531a64b"
   },
   "outputs": [],
   "source": [
    "def train_alphazero(model, white_strategy, board_size, iterations=100, games_per_iteration=10, \n",
    "                    batch_size=32, mcts_simulations=100, alpha=0.5, l2_lambda=1e-4, entropy_alpha=1e-3):\n",
    "    \"\"\"Train AlphaZero model\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "    replay_buffer = deque(maxlen=10000)  # Replay buffer to store training data\n",
    "    loss_fn_policy = nn.CrossEntropyLoss()\n",
    "    loss_fn_value = nn.MSELoss()\n",
    "\n",
    "    policy_loss_list = []\n",
    "    value_loss_list = []\n",
    "    results_list = [] \n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        # Generate self-play data\n",
    "        for _ in range(games_per_iteration):\n",
    "            game_data = self_play_game(model, board_size, mcts_simulations)\n",
    "            replay_buffer.extend(game_data)\n",
    "        \n",
    "        # Sample a batch from replay buffer\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            continue\n",
    "\n",
    "        batch = random.sample(replay_buffer, batch_size)\n",
    "        states, policies, values = zip(*batch)\n",
    "\n",
    "        # Move data to GPU\n",
    "        states = torch.tensor(np.stack(states), dtype=torch.float32).to(device)\n",
    "        policies = torch.tensor(np.stack(policies), dtype=torch.float32).view(batch_size, \n",
    "                                                        board_size * board_size).to(device)\n",
    "        values = torch.tensor(values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        pred_policies, pred_values = model(states)\n",
    "\n",
    "        # Compute losses\n",
    "        policy_loss = loss_fn_policy(pred_policies, policies)\n",
    "        value_loss = loss_fn_value(pred_values, values)\n",
    "\n",
    "        l2_reg = sum(param.pow(2).sum() for param in model.parameters())\n",
    "        #entropy = -torch.sum(pred_policies * torch.log(pred_policies + 1e-8), dim=1).mean()\n",
    "\n",
    "        loss = (policy_loss * alpha + value_loss * (1 - alpha)\n",
    "                + l2_lambda * l2_reg)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        policy_loss_list.append(policy_loss.item())\n",
    "        value_loss_list.append(value_loss.item())\n",
    "\n",
    "        print(f\"Iteration {iteration + 1}/{iterations}\")\n",
    "        print(f\"Policy Loss: {policy_loss.item():.4f}, Value Loss: {value_loss.item():.4f}\")\n",
    "\n",
    "        # Every 10 iterations, evaluate against pure MCTS\n",
    "        if (iteration + 1) % 10 == 0:\n",
    "            print(f\"Starting evaluation against pure MCTS at iteration {iteration + 1}\")\n",
    "            model_copy = copy.deepcopy(model)\n",
    "            wins, losses, ties = play_with_model_and_strategy(model_copy,\n",
    "                                                              white_strategy, board_size=board_size, rounds=10)\n",
    "            print(\"win: {}, lose: {}, tie:{}\".format(wins, losses, ties))\n",
    "            results_list.append((wins, losses, ties))\n",
    "\n",
    "    # Save model weights\n",
    "    torch.save(model.state_dict(), \"alphazero_weights.pth\")\n",
    "    print(\"Training complete. Weights saved to 'alphazero_weights.pth'.\")\n",
    "\n",
    "    return policy_loss_list, value_loss_list, results_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e7e0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "board_list = []\n",
    "def play_with_model_and_strategy(alpha_zero_model, white_strategy, board_size=8, rounds=1):\n",
    "    \"\"\"\n",
    "    使用 AlphaZero 模型和 white_strategy 策略进行对弈\n",
    "    Args:\n",
    "        alpha_zero_model: AlphaZero 模型实例\n",
    "        white_strategy: 白棋的策略函数\n",
    "        board_size: 棋盘大小\n",
    "        rounds: 对弈的轮数\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    # 将模型移动到 GPU\n",
    "    alpha_zero_model = alpha_zero_model.to(device)\n",
    "\n",
    "    scores = {'AlphaZero': 0, 'WhiteStrategy': 0, 'Draw': 0}\n",
    "    mcts = MCTS(alpha_zero_model,board_size)\n",
    "    for _ in range(rounds):\n",
    "        # 初始化棋盘和游戏状态\n",
    "        board = [['.' for _ in range(board_size)] for _ in range(board_size)]\n",
    "        current_player = 'X'  # AlphaZero 先手\n",
    "        winner = None\n",
    "        last_move = None\n",
    "\n",
    "        while True:\n",
    "            if current_player == 'X':  # AlphaZero 的回合\n",
    "                board_tensor = mcts.board_to_tensor(board, current_player,last_move).to(device)\n",
    "                best_action, _ = mcts.run(board,current_player)\n",
    "                #print(best_action)\n",
    "                x, y = best_action\n",
    "                move = x,y\n",
    "            else:  # white_strategy 的回合\n",
    "                move = white_strategy(board, 'O')\n",
    "\n",
    "                x, y = move\n",
    "\n",
    "            # 执行落子\n",
    "            if board[x][y] == '.':\n",
    "                board[x][y] = current_player\n",
    "\n",
    "                # 检查是否获胜\n",
    "                if mcts.check_winner(board, current_player):\n",
    "                    board_list.append(board)\n",
    "                    if current_player == 'X':\n",
    "                        scores['AlphaZero'] += 1\n",
    "                    else:\n",
    "                        scores['WhiteStrategy'] += 1\n",
    "                    break\n",
    "\n",
    "                # 切换玩家\n",
    "                current_player = 'O' if current_player == 'X' else 'X'\n",
    "            else:\n",
    "                #scores['WhiteStrategy' if current_player == 'X' else 'AlphaZero'] += 1\n",
    "                break\n",
    "            last_move = move\n",
    "\n",
    "        # 检查是否平局\n",
    "        if not any('.' in row for row in board) and winner is None:\n",
    "\n",
    "            scores['Draw'] += 1\n",
    "\n",
    "    # 打印对弈结果\n",
    "\n",
    "    return scores['AlphaZero'],scores['WhiteStrategy'],scores['Draw']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y0ZrFDw7iblB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 628229,
     "status": "ok",
     "timestamp": 1733702079186,
     "user": {
      "displayName": "Siyi Wu",
      "userId": "06196688456534608936"
     },
     "user_tz": 360
    },
    "id": "Y0ZrFDw7iblB",
    "outputId": "9da28a17-622b-4058-8bff-9520f6c32ad1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Iteration 4/800\n",
      "Policy Loss: 4.2169, Value Loss: 1.1306\n",
      "Iteration 5/800\n",
      "Policy Loss: 4.1832, Value Loss: 1.0171\n",
      "Iteration 6/800\n",
      "Policy Loss: 4.1603, Value Loss: 0.7339\n",
      "Iteration 7/800\n",
      "Policy Loss: 4.1284, Value Loss: 0.5369\n",
      "Iteration 8/800\n",
      "Policy Loss: 4.0972, Value Loss: 0.8261\n",
      "Iteration 9/800\n",
      "Policy Loss: 3.8171, Value Loss: 0.8624\n",
      "Iteration 10/800\n",
      "Policy Loss: 3.7120, Value Loss: 0.9876\n",
      "Starting evaluation against pure MCTS at iteration 10\n",
      "win: 0, lose: 10, tie:0\n",
      "Iteration 11/800\n",
      "Policy Loss: 3.5984, Value Loss: 0.9600\n",
      "Iteration 12/800\n",
      "Policy Loss: 3.6498, Value Loss: 0.3154\n",
      "Iteration 13/800\n",
      "Policy Loss: 2.9102, Value Loss: 0.4001\n",
      "Iteration 14/800\n",
      "Policy Loss: 3.9201, Value Loss: 0.2384\n",
      "Iteration 15/800\n",
      "Policy Loss: 3.3819, Value Loss: 0.4497\n",
      "Iteration 16/800\n",
      "Policy Loss: 3.1947, Value Loss: 0.5074\n",
      "Iteration 17/800\n",
      "Policy Loss: 3.9347, Value Loss: 0.3945\n",
      "Iteration 18/800\n",
      "Policy Loss: 3.5925, Value Loss: 0.2851\n",
      "Iteration 19/800\n",
      "Policy Loss: 3.2980, Value Loss: 0.1941\n",
      "Iteration 20/800\n",
      "Policy Loss: 3.4792, Value Loss: 0.0072\n",
      "Starting evaluation against pure MCTS at iteration 20\n",
      "win: 1, lose: 9, tie:0\n",
      "Iteration 21/800\n",
      "Policy Loss: 3.3398, Value Loss: 0.0610\n",
      "Iteration 22/800\n",
      "Policy Loss: 3.0467, Value Loss: 0.1223\n",
      "Iteration 23/800\n",
      "Policy Loss: 3.7444, Value Loss: 0.3663\n",
      "Iteration 24/800\n",
      "Policy Loss: 3.4765, Value Loss: 0.3469\n",
      "Iteration 25/800\n",
      "Policy Loss: 2.9067, Value Loss: 0.1706\n",
      "Iteration 26/800\n",
      "Policy Loss: 2.8585, Value Loss: 0.2259\n",
      "Iteration 27/800\n",
      "Policy Loss: 3.3786, Value Loss: 0.2811\n",
      "Iteration 28/800\n",
      "Policy Loss: 2.8307, Value Loss: 0.3343\n",
      "Iteration 29/800\n",
      "Policy Loss: 3.2739, Value Loss: 0.3331\n",
      "Iteration 30/800\n",
      "Policy Loss: 3.0259, Value Loss: 0.3308\n",
      "Starting evaluation against pure MCTS at iteration 30\n",
      "win: 1, lose: 9, tie:0\n",
      "Iteration 31/800\n",
      "Policy Loss: 2.7817, Value Loss: 0.2738\n",
      "Iteration 32/800\n",
      "Policy Loss: 2.8953, Value Loss: 0.4373\n",
      "Iteration 33/800\n",
      "Policy Loss: 3.5689, Value Loss: 0.4329\n",
      "Iteration 34/800\n",
      "Policy Loss: 2.8599, Value Loss: 0.4257\n",
      "Iteration 35/800\n",
      "Policy Loss: 3.2816, Value Loss: 0.4213\n",
      "Iteration 36/800\n",
      "Policy Loss: 2.9854, Value Loss: 0.3626\n",
      "Iteration 37/800\n",
      "Policy Loss: 3.0703, Value Loss: 0.4628\n",
      "Iteration 38/800\n",
      "Policy Loss: 3.1921, Value Loss: 0.3561\n",
      "Iteration 39/800\n",
      "Policy Loss: 3.3085, Value Loss: 0.2511\n",
      "Iteration 40/800\n",
      "Policy Loss: 3.4158, Value Loss: 0.6454\n",
      "Starting evaluation against pure MCTS at iteration 40\n",
      "win: 1, lose: 9, tie:0\n",
      "Iteration 41/800\n",
      "Policy Loss: 3.2093, Value Loss: 0.4897\n",
      "Iteration 42/800\n",
      "Policy Loss: 2.6648, Value Loss: 0.2418\n",
      "Iteration 43/800\n",
      "Policy Loss: 2.9908, Value Loss: 0.4768\n",
      "Iteration 44/800\n",
      "Policy Loss: 2.4431, Value Loss: 0.1882\n",
      "Iteration 45/800\n",
      "Policy Loss: 2.9627, Value Loss: 0.2779\n",
      "Iteration 46/800\n",
      "Policy Loss: 2.4935, Value Loss: 0.3650\n",
      "Iteration 47/800\n",
      "Policy Loss: 2.1895, Value Loss: 0.1806\n",
      "Iteration 48/800\n",
      "Policy Loss: 2.8828, Value Loss: 0.2665\n",
      "Iteration 49/800\n",
      "Policy Loss: 3.0484, Value Loss: 0.3511\n",
      "Iteration 50/800\n",
      "Policy Loss: 2.6717, Value Loss: 0.3003\n",
      "Starting evaluation against pure MCTS at iteration 50\n",
      "win: 0, lose: 10, tie:0\n",
      "Iteration 51/800\n",
      "Policy Loss: 2.2055, Value Loss: 0.2119\n",
      "Iteration 52/800\n",
      "Policy Loss: 2.3352, Value Loss: 0.2074\n",
      "Iteration 53/800\n",
      "Policy Loss: 3.2857, Value Loss: 0.3237\n",
      "Iteration 54/800\n",
      "Policy Loss: 2.8232, Value Loss: 0.1633\n",
      "Iteration 55/800\n",
      "Policy Loss: 2.0790, Value Loss: 0.2352\n",
      "Iteration 56/800\n",
      "Policy Loss: 2.5800, Value Loss: 0.1545\n",
      "Iteration 57/800\n",
      "Policy Loss: 2.1791, Value Loss: 0.2230\n",
      "Iteration 58/800\n",
      "Policy Loss: 2.4629, Value Loss: 0.1850\n",
      "Iteration 59/800\n",
      "Policy Loss: 2.3217, Value Loss: 0.1043\n",
      "Iteration 60/800\n",
      "Policy Loss: 2.7309, Value Loss: 0.2808\n",
      "Starting evaluation against pure MCTS at iteration 60\n",
      "win: 0, lose: 10, tie:0\n",
      "Iteration 61/800\n",
      "Policy Loss: 2.6472, Value Loss: 0.3098\n",
      "Iteration 62/800\n",
      "Policy Loss: 1.7971, Value Loss: 0.1579\n",
      "Iteration 63/800\n",
      "Policy Loss: 2.5008, Value Loss: 0.1600\n",
      "Iteration 64/800\n",
      "Policy Loss: 1.9228, Value Loss: 0.2641\n",
      "Iteration 65/800\n",
      "Policy Loss: 2.1990, Value Loss: 0.1765\n",
      "Iteration 66/800\n",
      "Policy Loss: 2.4454, Value Loss: 0.1626\n",
      "Iteration 67/800\n",
      "Policy Loss: 1.8432, Value Loss: 0.0922\n",
      "Iteration 68/800\n",
      "Policy Loss: 2.1901, Value Loss: 0.1587\n",
      "Iteration 69/800\n",
      "Policy Loss: 2.4631, Value Loss: 0.0768\n",
      "Iteration 70/800\n",
      "Policy Loss: 1.9144, Value Loss: 0.0904\n",
      "Starting evaluation against pure MCTS at iteration 70\n",
      "win: 7, lose: 3, tie:0\n",
      "Iteration 71/800\n",
      "Policy Loss: 2.1391, Value Loss: 0.1522\n",
      "Iteration 72/800\n",
      "Policy Loss: 2.0370, Value Loss: 0.0724\n",
      "Iteration 73/800\n",
      "Policy Loss: 1.9585, Value Loss: 0.0836\n",
      "Iteration 74/800\n",
      "Policy Loss: 2.1390, Value Loss: 0.0662\n",
      "Iteration 75/800\n",
      "Policy Loss: 2.0661, Value Loss: 0.0251\n",
      "Iteration 76/800\n",
      "Policy Loss: 2.0381, Value Loss: 0.0229\n",
      "Iteration 77/800\n",
      "Policy Loss: 1.5026, Value Loss: 0.0163\n",
      "Iteration 78/800\n",
      "Policy Loss: 1.3933, Value Loss: 0.0005\n",
      "Iteration 79/800\n",
      "Policy Loss: 1.5078, Value Loss: 0.0042\n",
      "Iteration 80/800\n",
      "Policy Loss: 1.5100, Value Loss: 0.0033\n",
      "Starting evaluation against pure MCTS at iteration 80\n",
      "win: 6, lose: 4, tie:0\n",
      "Iteration 81/800\n",
      "Policy Loss: 1.7342, Value Loss: 0.0014\n",
      "Iteration 82/800\n",
      "Policy Loss: 1.3303, Value Loss: 0.0003\n",
      "Iteration 83/800\n",
      "Policy Loss: 1.5152, Value Loss: 0.0013\n",
      "Iteration 84/800\n",
      "Policy Loss: 1.2298, Value Loss: 0.0672\n",
      "Iteration 85/800\n",
      "Policy Loss: 1.1662, Value Loss: 0.0009\n",
      "Iteration 86/800\n",
      "Policy Loss: 1.3021, Value Loss: 0.0040\n",
      "Iteration 87/800\n",
      "Policy Loss: 1.6860, Value Loss: 0.0547\n",
      "Iteration 88/800\n",
      "Policy Loss: 1.4358, Value Loss: 0.0218\n",
      "Iteration 89/800\n",
      "Policy Loss: 1.2672, Value Loss: 0.0233\n",
      "Iteration 90/800\n",
      "Policy Loss: 1.0098, Value Loss: 0.0005\n",
      "Starting evaluation against pure MCTS at iteration 90\n",
      "win: 4, lose: 6, tie:0\n",
      "Iteration 91/800\n",
      "Policy Loss: 1.3423, Value Loss: 0.0007\n",
      "Iteration 92/800\n",
      "Policy Loss: 1.7181, Value Loss: 0.0007\n",
      "Iteration 93/800\n",
      "Policy Loss: 1.1883, Value Loss: 0.0004\n",
      "Iteration 94/800\n",
      "Policy Loss: 1.1402, Value Loss: 0.0000\n",
      "Iteration 95/800\n",
      "Policy Loss: 0.8589, Value Loss: 0.0001\n",
      "Iteration 96/800\n",
      "Policy Loss: 1.1069, Value Loss: 0.0000\n",
      "Iteration 97/800\n",
      "Policy Loss: 1.2824, Value Loss: 0.0002\n",
      "Iteration 98/800\n",
      "Policy Loss: 1.0006, Value Loss: 0.0001\n",
      "Iteration 99/800\n",
      "Policy Loss: 0.6276, Value Loss: 0.0000\n",
      "Iteration 100/800\n",
      "Policy Loss: 0.7278, Value Loss: 0.0001\n",
      "Starting evaluation against pure MCTS at iteration 100\n",
      "win: 3, lose: 7, tie:0\n",
      "Iteration 101/800\n",
      "Policy Loss: 0.9134, Value Loss: 0.0001\n",
      "Iteration 102/800\n",
      "Policy Loss: 0.9768, Value Loss: 0.0001\n",
      "Iteration 103/800\n",
      "Policy Loss: 0.8562, Value Loss: 0.0000\n",
      "Iteration 104/800\n",
      "Policy Loss: 0.7361, Value Loss: 0.0000\n",
      "Iteration 105/800\n",
      "Policy Loss: 0.6270, Value Loss: 0.0000\n",
      "Iteration 106/800\n",
      "Policy Loss: 0.4180, Value Loss: 0.0000\n",
      "Iteration 107/800\n",
      "Policy Loss: 0.3368, Value Loss: 0.0000\n",
      "Iteration 108/800\n",
      "Policy Loss: 0.6761, Value Loss: 0.0000\n",
      "Iteration 109/800\n",
      "Policy Loss: 0.3791, Value Loss: 0.0000\n",
      "Iteration 110/800\n",
      "Policy Loss: 0.2823, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 110\n",
      "win: 1, lose: 9, tie:0\n",
      "Iteration 111/800\n",
      "Policy Loss: 0.4184, Value Loss: 0.0000\n",
      "Iteration 112/800\n",
      "Policy Loss: 0.5251, Value Loss: 0.0000\n",
      "Iteration 113/800\n",
      "Policy Loss: 0.6874, Value Loss: 0.0003\n",
      "Iteration 114/800\n",
      "Policy Loss: 0.5748, Value Loss: 0.0000\n",
      "Iteration 115/800\n",
      "Policy Loss: 0.6206, Value Loss: 0.0000\n",
      "Iteration 116/800\n",
      "Policy Loss: 0.2838, Value Loss: 0.0000\n",
      "Iteration 117/800\n",
      "Policy Loss: 0.7307, Value Loss: 0.0002\n",
      "Iteration 118/800\n",
      "Policy Loss: 0.1195, Value Loss: 0.0000\n",
      "Iteration 119/800\n",
      "Policy Loss: 0.3884, Value Loss: 0.0002\n",
      "Iteration 120/800\n",
      "Policy Loss: 0.2542, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 120\n",
      "win: 6, lose: 4, tie:0\n",
      "Iteration 121/800\n",
      "Policy Loss: 0.2343, Value Loss: 0.0000\n",
      "Iteration 122/800\n",
      "Policy Loss: 0.4781, Value Loss: 0.0000\n",
      "Iteration 123/800\n",
      "Policy Loss: 0.1635, Value Loss: 0.0033\n",
      "Iteration 124/800\n",
      "Policy Loss: 0.1211, Value Loss: 0.0000\n",
      "Iteration 125/800\n",
      "Policy Loss: 0.2413, Value Loss: 0.0000\n",
      "Iteration 126/800\n",
      "Policy Loss: 0.2968, Value Loss: 0.0000\n",
      "Iteration 127/800\n",
      "Policy Loss: 0.3755, Value Loss: 0.0000\n",
      "Iteration 128/800\n",
      "Policy Loss: 0.6229, Value Loss: 0.0008\n",
      "Iteration 129/800\n",
      "Policy Loss: 0.2191, Value Loss: 0.0000\n",
      "Iteration 130/800\n",
      "Policy Loss: 0.0787, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win: 7, lose: 3, tie:0\n",
      "Iteration 131/800\n",
      "Policy Loss: 0.7628, Value Loss: 0.0000\n",
      "Iteration 132/800\n",
      "Policy Loss: 0.1655, Value Loss: 0.0012\n",
      "Iteration 133/800\n",
      "Policy Loss: 0.3287, Value Loss: 0.0000\n",
      "Iteration 134/800\n",
      "Policy Loss: 0.5364, Value Loss: 0.0000\n",
      "Iteration 135/800\n",
      "Policy Loss: 0.4540, Value Loss: 0.0038\n",
      "Iteration 136/800\n",
      "Policy Loss: 0.2450, Value Loss: 0.0000\n",
      "Iteration 137/800\n",
      "Policy Loss: 0.2495, Value Loss: 0.0000\n",
      "Iteration 138/800\n",
      "Policy Loss: 0.1996, Value Loss: 0.0000\n",
      "Iteration 139/800\n",
      "Policy Loss: 0.0180, Value Loss: 0.0000\n",
      "Iteration 140/800\n",
      "Policy Loss: 0.3464, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 140\n",
      "win: 4, lose: 6, tie:0\n",
      "Iteration 141/800\n",
      "Policy Loss: 0.2599, Value Loss: 0.0001\n",
      "Iteration 142/800\n",
      "Policy Loss: 0.1412, Value Loss: 0.0132\n",
      "Iteration 143/800\n",
      "Policy Loss: 0.0542, Value Loss: 0.0000\n",
      "Iteration 144/800\n",
      "Policy Loss: 0.0979, Value Loss: 0.0000\n",
      "Iteration 145/800\n",
      "Policy Loss: 0.1877, Value Loss: 0.0000\n",
      "Iteration 146/800\n",
      "Policy Loss: 0.0610, Value Loss: 0.0002\n",
      "Iteration 147/800\n",
      "Policy Loss: 0.0198, Value Loss: 0.0000\n",
      "Iteration 148/800\n",
      "Policy Loss: 0.1406, Value Loss: 0.0000\n",
      "Iteration 149/800\n",
      "Policy Loss: 0.1231, Value Loss: 0.0000\n",
      "Iteration 150/800\n",
      "Policy Loss: 0.0609, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 150\n",
      "win: 2, lose: 8, tie:0\n",
      "Iteration 151/800\n",
      "Policy Loss: 0.0261, Value Loss: 0.0000\n",
      "Iteration 152/800\n",
      "Policy Loss: 0.0935, Value Loss: 0.0000\n",
      "Iteration 153/800\n",
      "Policy Loss: 0.0599, Value Loss: 0.0000\n",
      "Iteration 154/800\n",
      "Policy Loss: 0.0599, Value Loss: 0.0000\n",
      "Iteration 155/800\n",
      "Policy Loss: 0.0186, Value Loss: 0.0000\n",
      "Iteration 156/800\n",
      "Policy Loss: 0.0230, Value Loss: 0.0000\n",
      "Iteration 157/800\n",
      "Policy Loss: 0.0317, Value Loss: 0.0000\n",
      "Iteration 158/800\n",
      "Policy Loss: 0.1629, Value Loss: 0.0000\n",
      "Iteration 159/800\n",
      "Policy Loss: 0.0394, Value Loss: 0.0000\n",
      "Iteration 160/800\n",
      "Policy Loss: 0.0996, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 160\n",
      "win: 6, lose: 4, tie:0\n",
      "Iteration 161/800\n",
      "Policy Loss: 0.0171, Value Loss: 0.0000\n",
      "Iteration 162/800\n",
      "Policy Loss: 0.0232, Value Loss: 0.0000\n",
      "Iteration 163/800\n",
      "Policy Loss: 0.0175, Value Loss: 0.0000\n",
      "Iteration 164/800\n",
      "Policy Loss: 0.0336, Value Loss: 0.0000\n",
      "Iteration 165/800\n",
      "Policy Loss: 0.0315, Value Loss: 0.0000\n",
      "Iteration 166/800\n",
      "Policy Loss: 0.0090, Value Loss: 0.0000\n",
      "Iteration 167/800\n",
      "Policy Loss: 0.0438, Value Loss: 0.0000\n",
      "Iteration 168/800\n",
      "Policy Loss: 0.0058, Value Loss: 0.0000\n",
      "Iteration 169/800\n",
      "Policy Loss: 0.0226, Value Loss: 0.0000\n",
      "Iteration 170/800\n",
      "Policy Loss: 0.0022, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 170\n",
      "win: 7, lose: 3, tie:0\n",
      "Iteration 171/800\n",
      "Policy Loss: 0.0231, Value Loss: 0.0000\n",
      "Iteration 172/800\n",
      "Policy Loss: 0.0362, Value Loss: 0.0000\n",
      "Iteration 173/800\n",
      "Policy Loss: 0.0161, Value Loss: 0.0000\n",
      "Iteration 174/800\n",
      "Policy Loss: 0.0030, Value Loss: 0.0000\n",
      "Iteration 175/800\n",
      "Policy Loss: 0.0516, Value Loss: 0.0000\n",
      "Iteration 176/800\n",
      "Policy Loss: 0.0122, Value Loss: 0.0000\n",
      "Iteration 177/800\n",
      "Policy Loss: 0.0058, Value Loss: 0.0000\n",
      "Iteration 178/800\n",
      "Policy Loss: 0.0028, Value Loss: 0.0000\n",
      "Iteration 179/800\n",
      "Policy Loss: 0.0196, Value Loss: 0.0000\n",
      "Iteration 180/800\n",
      "Policy Loss: 0.0024, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 180\n",
      "win: 8, lose: 2, tie:0\n",
      "Iteration 181/800\n",
      "Policy Loss: 0.0162, Value Loss: 0.0000\n",
      "Iteration 182/800\n",
      "Policy Loss: 0.0144, Value Loss: 0.0000\n",
      "Iteration 183/800\n",
      "Policy Loss: 0.0014, Value Loss: 0.0000\n",
      "Iteration 184/800\n",
      "Policy Loss: 0.0027, Value Loss: 0.0000\n",
      "Iteration 185/800\n",
      "Policy Loss: 0.0065, Value Loss: 0.0000\n",
      "Iteration 186/800\n",
      "Policy Loss: 0.0070, Value Loss: 0.0000\n",
      "Iteration 187/800\n",
      "Policy Loss: 0.0038, Value Loss: 0.0000\n",
      "Iteration 188/800\n",
      "Policy Loss: 0.0086, Value Loss: 0.0000\n",
      "Iteration 189/800\n",
      "Policy Loss: 0.0019, Value Loss: 0.0000\n",
      "Iteration 190/800\n",
      "Policy Loss: 0.0015, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 190\n",
      "win: 7, lose: 3, tie:0\n",
      "Iteration 191/800\n",
      "Policy Loss: 0.0009, Value Loss: 0.0000\n",
      "Iteration 192/800\n",
      "Policy Loss: 0.0022, Value Loss: 0.0000\n",
      "Iteration 193/800\n",
      "Policy Loss: 0.0004, Value Loss: 0.0000\n",
      "Iteration 194/800\n",
      "Policy Loss: 0.0019, Value Loss: 0.0000\n",
      "Iteration 195/800\n",
      "Policy Loss: 0.0033, Value Loss: 0.0000\n",
      "Iteration 196/800\n",
      "Policy Loss: 0.0005, Value Loss: 0.0000\n",
      "Iteration 197/800\n",
      "Policy Loss: 0.0018, Value Loss: 0.0000\n",
      "Iteration 198/800\n",
      "Policy Loss: 0.0011, Value Loss: 0.0000\n",
      "Iteration 199/800\n",
      "Policy Loss: 0.0048, Value Loss: 0.0000\n",
      "Iteration 200/800\n",
      "Policy Loss: 0.0006, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 200\n",
      "win: 8, lose: 2, tie:0\n",
      "Iteration 201/800\n",
      "Policy Loss: 0.0031, Value Loss: 0.0000\n",
      "Iteration 202/800\n",
      "Policy Loss: 0.0023, Value Loss: 0.0000\n",
      "Iteration 203/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 204/800\n",
      "Policy Loss: 0.0008, Value Loss: 0.0000\n",
      "Iteration 205/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 206/800\n",
      "Policy Loss: 0.0006, Value Loss: 0.0000\n",
      "Iteration 207/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 208/800\n",
      "Policy Loss: 0.0006, Value Loss: 0.0000\n",
      "Iteration 209/800\n",
      "Policy Loss: 0.0004, Value Loss: 0.0000\n",
      "Iteration 210/800\n",
      "Policy Loss: 0.0005, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 210\n",
      "win: 6, lose: 4, tie:0\n",
      "Iteration 211/800\n",
      "Policy Loss: 0.0106, Value Loss: 0.0000\n",
      "Iteration 212/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 213/800\n",
      "Policy Loss: 0.0023, Value Loss: 0.0000\n",
      "Iteration 214/800\n",
      "Policy Loss: 0.0012, Value Loss: 0.0000\n",
      "Iteration 215/800\n",
      "Policy Loss: 0.0004, Value Loss: 0.0000\n",
      "Iteration 216/800\n",
      "Policy Loss: 0.0030, Value Loss: 0.0000\n",
      "Iteration 217/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 218/800\n",
      "Policy Loss: 0.0005, Value Loss: 0.0000\n",
      "Iteration 219/800\n",
      "Policy Loss: 0.0005, Value Loss: 0.0000\n",
      "Iteration 220/800\n",
      "Policy Loss: 0.0005, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 220\n",
      "win: 6, lose: 4, tie:0\n",
      "Iteration 221/800\n",
      "Policy Loss: 0.0007, Value Loss: 0.0000\n",
      "Iteration 222/800\n",
      "Policy Loss: 0.0012, Value Loss: 0.0000\n",
      "Iteration 223/800\n",
      "Policy Loss: 0.0006, Value Loss: 0.0000\n",
      "Iteration 224/800\n",
      "Policy Loss: 0.0007, Value Loss: 0.0000\n",
      "Iteration 225/800\n",
      "Policy Loss: 0.0011, Value Loss: 0.0000\n",
      "Iteration 226/800\n",
      "Policy Loss: 0.0011, Value Loss: 0.0000\n",
      "Iteration 227/800\n",
      "Policy Loss: 0.0006, Value Loss: 0.0000\n",
      "Iteration 228/800\n",
      "Policy Loss: 0.0010, Value Loss: 0.0000\n",
      "Iteration 229/800\n",
      "Policy Loss: 0.0008, Value Loss: 0.0000\n",
      "Iteration 230/800\n",
      "Policy Loss: 0.0005, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 230\n",
      "win: 8, lose: 2, tie:0\n",
      "Iteration 231/800\n",
      "Policy Loss: 0.0007, Value Loss: 0.0000\n",
      "Iteration 232/800\n",
      "Policy Loss: 0.0014, Value Loss: 0.0000\n",
      "Iteration 233/800\n",
      "Policy Loss: 0.0007, Value Loss: 0.0000\n",
      "Iteration 234/800\n",
      "Policy Loss: 0.0006, Value Loss: 0.0000\n",
      "Iteration 235/800\n",
      "Policy Loss: 0.0015, Value Loss: 0.0000\n",
      "Iteration 236/800\n",
      "Policy Loss: 0.0013, Value Loss: 0.0000\n",
      "Iteration 237/800\n",
      "Policy Loss: 0.0007, Value Loss: 0.0000\n",
      "Iteration 238/800\n",
      "Policy Loss: 0.0005, Value Loss: 0.0000\n",
      "Iteration 239/800\n",
      "Policy Loss: 0.0007, Value Loss: 0.0000\n",
      "Iteration 240/800\n",
      "Policy Loss: 0.0004, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 240\n",
      "win: 6, lose: 4, tie:0\n",
      "Iteration 241/800\n",
      "Policy Loss: 0.0006, Value Loss: 0.0000\n",
      "Iteration 242/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 243/800\n",
      "Policy Loss: 0.0002, Value Loss: 0.0000\n",
      "Iteration 244/800\n",
      "Policy Loss: 0.0012, Value Loss: 0.0000\n",
      "Iteration 245/800\n",
      "Policy Loss: 0.0006, Value Loss: 0.0000\n",
      "Iteration 246/800\n",
      "Policy Loss: 0.0010, Value Loss: 0.0000\n",
      "Iteration 247/800\n",
      "Policy Loss: 0.0004, Value Loss: 0.0000\n",
      "Iteration 248/800\n",
      "Policy Loss: 0.0004, Value Loss: 0.0000\n",
      "Iteration 249/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 250/800\n",
      "Policy Loss: 0.0004, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 250\n",
      "win: 9, lose: 1, tie:0\n",
      "Iteration 251/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 252/800\n",
      "Policy Loss: 0.0011, Value Loss: 0.0000\n",
      "Iteration 253/800\n",
      "Policy Loss: 0.0006, Value Loss: 0.0000\n",
      "Iteration 254/800\n",
      "Policy Loss: 0.0004, Value Loss: 0.0000\n",
      "Iteration 255/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256/800\n",
      "Policy Loss: 0.0004, Value Loss: 0.0000\n",
      "Iteration 257/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 258/800\n",
      "Policy Loss: 0.0005, Value Loss: 0.0000\n",
      "Iteration 259/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 260/800\n",
      "Policy Loss: 0.0004, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 260\n",
      "win: 5, lose: 5, tie:0\n",
      "Iteration 261/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 262/800\n",
      "Policy Loss: 0.0002, Value Loss: 0.0000\n",
      "Iteration 263/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 264/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 265/800\n",
      "Policy Loss: 0.0011, Value Loss: 0.0000\n",
      "Iteration 266/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 267/800\n",
      "Policy Loss: 0.0004, Value Loss: 0.0000\n",
      "Iteration 268/800\n",
      "Policy Loss: 0.0004, Value Loss: 0.0000\n",
      "Iteration 269/800\n",
      "Policy Loss: 0.0002, Value Loss: 0.0000\n",
      "Iteration 270/800\n",
      "Policy Loss: 0.0004, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 270\n",
      "win: 9, lose: 1, tie:0\n",
      "Iteration 271/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 272/800\n",
      "Policy Loss: 0.0002, Value Loss: 0.0000\n",
      "Iteration 273/800\n",
      "Policy Loss: 0.0011, Value Loss: 0.0000\n",
      "Iteration 274/800\n",
      "Policy Loss: 0.0002, Value Loss: 0.0000\n",
      "Iteration 275/800\n",
      "Policy Loss: 0.0002, Value Loss: 0.0000\n",
      "Iteration 276/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 277/800\n",
      "Policy Loss: 0.0004, Value Loss: 0.0000\n",
      "Iteration 278/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 279/800\n",
      "Policy Loss: 0.0001, Value Loss: 0.0000\n",
      "Iteration 280/800\n",
      "Policy Loss: 0.0004, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 280\n",
      "win: 6, lose: 4, tie:0\n",
      "Iteration 281/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 282/800\n",
      "Policy Loss: 0.0002, Value Loss: 0.0000\n",
      "Iteration 283/800\n",
      "Policy Loss: 0.0002, Value Loss: 0.0000\n",
      "Iteration 284/800\n",
      "Policy Loss: 0.0002, Value Loss: 0.0000\n",
      "Iteration 285/800\n",
      "Policy Loss: 0.0002, Value Loss: 0.0000\n",
      "Iteration 286/800\n",
      "Policy Loss: 0.0004, Value Loss: 0.0000\n",
      "Iteration 287/800\n",
      "Policy Loss: 0.0002, Value Loss: 0.0000\n",
      "Iteration 288/800\n",
      "Policy Loss: 0.0009, Value Loss: 0.0000\n",
      "Iteration 289/800\n",
      "Policy Loss: 0.0001, Value Loss: 0.0000\n",
      "Iteration 290/800\n",
      "Policy Loss: 0.0005, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 290\n",
      "win: 7, lose: 3, tie:0\n",
      "Iteration 291/800\n",
      "Policy Loss: 0.0001, Value Loss: 0.0000\n",
      "Iteration 292/800\n",
      "Policy Loss: 0.0002, Value Loss: 0.0000\n",
      "Iteration 293/800\n",
      "Policy Loss: 0.0002, Value Loss: 0.0000\n",
      "Iteration 294/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 295/800\n",
      "Policy Loss: 0.0008, Value Loss: 0.0000\n",
      "Iteration 296/800\n",
      "Policy Loss: 0.0002, Value Loss: 0.0000\n",
      "Iteration 297/800\n",
      "Policy Loss: 0.0002, Value Loss: 0.0000\n",
      "Iteration 298/800\n",
      "Policy Loss: 0.0010, Value Loss: 0.0000\n",
      "Iteration 299/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n",
      "Iteration 300/800\n",
      "Policy Loss: 0.0001, Value Loss: 0.0000\n",
      "Starting evaluation against pure MCTS at iteration 300\n",
      "win: 6, lose: 4, tie:0\n",
      "Iteration 301/800\n",
      "Policy Loss: 0.0004, Value Loss: 0.0000\n",
      "Iteration 302/800\n",
      "Policy Loss: 0.0002, Value Loss: 0.0000\n",
      "Iteration 303/800\n",
      "Policy Loss: 0.0002, Value Loss: 0.0000\n",
      "Iteration 304/800\n",
      "Policy Loss: 0.0001, Value Loss: 0.0000\n",
      "Iteration 305/800\n",
      "Policy Loss: 0.0003, Value Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "board_size = 8\n",
    "model = AlphaZeroNet(board_size)\n",
    "white_strategy = load_strategy('./mcts_strategy.py')\n",
    "\n",
    "policy_loss, value_loss, results_list = train_alphazero(\n",
    "                        model=model,\n",
    "                        white_strategy = white_strategy,\n",
    "                        board_size=board_size,\n",
    "                        iterations=800,\n",
    "                        games_per_iteration=5,\n",
    "                        batch_size=16,\n",
    "                        mcts_simulations=50,\n",
    "                        alpha=0.5,\n",
    "                        l2_lambda=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "qkRUTTmdsouH",
   "metadata": {
    "id": "qkRUTTmdsouH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded successfully!\n",
      "Starting 10 rounds of matches...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 开始对弈\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrounds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rounds of matches...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m alpha_zero_wins, white_strategy_wins, draws \u001b[38;5;241m=\u001b[39m \u001b[43mplay_with_model_and_strategy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhite_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboard_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboard_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, lose: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, tie:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(alpha_zero_wins, white_strategy_wins, draws))\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mplay_with_model_and_strategy\u001b[0;34m(alpha_zero_model, white_strategy, board_size, rounds)\u001b[0m\n\u001b[1;32m     33\u001b[0m     move \u001b[38;5;241m=\u001b[39m x,y\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# white_strategy 的回合\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     move \u001b[38;5;241m=\u001b[39m \u001b[43mwhite_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mO\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m move\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# 执行落子\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Gomoku/mctsv2_strategy.py:153\u001b[0m, in \u001b[0;36mplay\u001b[0;34m(board, player, device)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplay\u001b[39m(board, player,device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    152\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"基于 MCTS 的策略\"\"\"\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmcts_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Gomoku/mctsv2_strategy.py:101\u001b[0m, in \u001b[0;36mmcts_search\u001b[0;34m(initial_board, player, iterations)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m move \u001b[38;5;129;01min\u001b[39;00m legal_moves:\n\u001b[1;32m    100\u001b[0m     new_state \u001b[38;5;241m=\u001b[39m make_move(state, move, player)\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcauses_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    102\u001b[0m         child_node \u001b[38;5;241m=\u001b[39m MCTSNode(new_state, node)\n\u001b[1;32m    103\u001b[0m         node\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mappend(child_node)\n",
      "File \u001b[0;32m~/Desktop/Gomoku/mctsv2_strategy.py:145\u001b[0m, in \u001b[0;36mcauses_loss\u001b[0;34m(board, player)\u001b[0m\n\u001b[1;32m    143\u001b[0m moves \u001b[38;5;241m=\u001b[39m get_legal_moves(board)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m move \u001b[38;5;129;01min\u001b[39;00m moves:\n\u001b[0;32m--> 145\u001b[0m     simulated_board \u001b[38;5;241m=\u001b[39m \u001b[43mmake_move\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmove\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopponent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_winner(simulated_board, opponent):\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# 对手可以在下一步获胜，避免这个位置\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Gomoku/mctsv2_strategy.py:41\u001b[0m, in \u001b[0;36mmake_move\u001b[0;34m(board, move, player)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_move\u001b[39m(board, move, player):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"模拟在棋盘上执行一步棋\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     new_board \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m move\n\u001b[1;32m     43\u001b[0m     new_board[x][y] \u001b[38;5;241m=\u001b[39m player\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/copy.py:205\u001b[0m, in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    203\u001b[0m append \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mappend\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[0;32m--> 205\u001b[0m     append(\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/copy.py:202\u001b[0m, in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_deepcopy_list\u001b[39m(x, memo, deepcopy\u001b[38;5;241m=\u001b[39mdeepcopy):\n\u001b[1;32m    201\u001b[0m     y \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 202\u001b[0m     memo[\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    203\u001b[0m     append \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mappend\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m x:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "board_size = 7\n",
    "rounds = 10  # 对弈的轮数\n",
    "weights_path = \"./weights/alphazero_weights.pth\"  # AlphaZero 模型权重文件路径\n",
    "#weights_path = \"./weights_v2/size8_win5\"  # AlphaZero 模型权重文件路径\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 定义 white_strategy，使用简单的随机策略\n",
    "white_strategy = load_strategy('./mctsv2_strategy.py')\n",
    "\n",
    "\n",
    "model = AlphaZeroNet(board_size)\n",
    "\n",
    "# 获取模型的 state_dict\n",
    "\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "except:\n",
    "    state_dict = model.state_dict()\n",
    "# 将权重从 list 转换为 PyTorch 张量并加载到模型\n",
    "    for (name, param), weight in zip(state_dict.items(), policy_param):\n",
    "        print(f\"Loading {name} with shape {weight.shape}\")\n",
    "        if \"weight\" in name and param.shape != torch.tensor(weight).shape:\n",
    "            weight = weight.T  # 转置权重\n",
    "        state_dict[name] = torch.tensor(weight, dtype=param.dtype)\n",
    "\n",
    "# 加载更新后的 state_dict 到模型\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "print(\"Model weights loaded successfully!\")\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 开始对弈\n",
    "print(f\"Starting {rounds} rounds of matches...\")\n",
    "alpha_zero_wins, white_strategy_wins, draws = play_with_model_and_strategy(\n",
    "    model,\n",
    "    white_strategy,\n",
    "    board_size=board_size,\n",
    "    rounds=rounds,\n",
    ")\n",
    "\n",
    "print(\"win: {}, lose: {}, tie:{}\".format(alpha_zero_wins, white_strategy_wins, draws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ab358fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded successfully!\n",
      "Starting 10 rounds of matches...\n",
      "win: 6, lose: 4, tie:0\n"
     ]
    }
   ],
   "source": [
    "board_size = 7\n",
    "rounds = 10  # 对弈的轮数\n",
    "weights_path = \"./weights/alphazero_weights.pth\"  # AlphaZero 模型权重文件路径\n",
    "#weights_path = \"./weights_v2/size8_win5\"  # AlphaZero 模型权重文件路径\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 定义 white_strategy，使用简单的随机策略\n",
    "white_strategy = load_strategy('./mcts_strategy.py')\n",
    "\n",
    "\n",
    "model = AlphaZeroNet(board_size)\n",
    "\n",
    "# 获取模型的 state_dict\n",
    "\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "except:\n",
    "    state_dict = model.state_dict()\n",
    "# 将权重从 list 转换为 PyTorch 张量并加载到模型\n",
    "    for (name, param), weight in zip(state_dict.items(), policy_param):\n",
    "        print(f\"Loading {name} with shape {weight.shape}\")\n",
    "        if \"weight\" in name and param.shape != torch.tensor(weight).shape:\n",
    "            weight = weight.T  # 转置权重\n",
    "        state_dict[name] = torch.tensor(weight, dtype=param.dtype)\n",
    "\n",
    "# 加载更新后的 state_dict 到模型\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "print(\"Model weights loaded successfully!\")\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 开始对弈\n",
    "print(f\"Starting {rounds} rounds of matches...\")\n",
    "alpha_zero_wins, white_strategy_wins, draws = play_with_model_and_strategy(\n",
    "    model,\n",
    "    white_strategy,\n",
    "    board_size=board_size,\n",
    "    rounds=rounds,\n",
    ")\n",
    "\n",
    "print(\"win: {}, lose: {}, tie:{}\".format(alpha_zero_wins, white_strategy_wins, draws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a30653eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import turtle\n",
    "\n",
    "def draw_board(size, cell_size):\n",
    "    \"\"\"绘制棋盘\"\"\"\n",
    "    turtle.speed(0)\n",
    "    turtle.penup()\n",
    "    turtle.hideturtle()\n",
    "    turtle.bgcolor(\"saddlebrown\")  # 棋盘底色设置为咖啡色\n",
    "\n",
    "    # 画棋盘网格线\n",
    "    for i in range(size):\n",
    "        # 画水平线\n",
    "        turtle.goto(-size * cell_size / 2, size * cell_size / 2 - i * cell_size)\n",
    "        turtle.pendown()\n",
    "        turtle.forward((size - 1) * cell_size)\n",
    "        turtle.penup()\n",
    "        # 画垂直线\n",
    "        turtle.goto(-size * cell_size / 2 + i * cell_size, size * cell_size / 2)\n",
    "        turtle.setheading(-90)\n",
    "        turtle.pendown()\n",
    "        turtle.forward((size - 1) * cell_size)\n",
    "        turtle.penup()\n",
    "        turtle.setheading(0)\n",
    "\n",
    "def draw_X(x, y, cell_size):\n",
    "    \"\"\"在指定位置绘制黑色棋子\"\"\"\n",
    "    turtle.penup()\n",
    "    turtle.goto(x * cell_size - cell_size / 2, y * cell_size - cell_size / 2)\n",
    "    turtle.dot(cell_size * 0.6, \"black\")  # 棋子大小为格子的60%\n",
    "\n",
    "def draw_O(x, y, cell_size):\n",
    "    \"\"\"在指定位置绘制白色棋子\"\"\"\n",
    "    turtle.penup()\n",
    "    turtle.goto(x * cell_size - cell_size / 2, y * cell_size - cell_size / 2)\n",
    "    turtle.dot(cell_size * 0.6, \"white\")  # 棋子大小为格子的60%\n",
    "\n",
    "def visualize_board(board):\n",
    "    \"\"\"\n",
    "    根据棋盘状态绘制棋盘和棋子\n",
    "    Args:\n",
    "        board: 棋盘，二维列表，'.'为空，'X'为黑棋，'O'为白棋\n",
    "    \"\"\"\n",
    "    size = len(board)\n",
    "    cell_size = 50  # 每个格子的大小\n",
    "\n",
    "    # 设置窗口大小\n",
    "    turtle.setup(width=size * cell_size + 100, height=size * cell_size + 100)\n",
    "    turtle.setworldcoordinates(-(size - 1) / 2 * cell_size, \n",
    "                                -(size - 1) / 2 * cell_size,\n",
    "                                (size + 1) / 2 * cell_size,\n",
    "                                (size + 1) / 2 * cell_size)\n",
    "\n",
    "    # 绘制棋盘\n",
    "    draw_board(size, cell_size)\n",
    "\n",
    "    # 绘制棋子\n",
    "    for y, row in enumerate(board):\n",
    "        for x, cell in enumerate(row):\n",
    "            # x 和 y 的位置调整，以棋盘格点交叉点为基准\n",
    "            x_pos = x - (size - 1) / 2\n",
    "            y_pos = (size - 1) / 2 - y\n",
    "            if cell == 'X':\n",
    "                draw_X(x_pos, y_pos, cell_size)\n",
    "            elif cell == 'O':\n",
    "                draw_O(x_pos, y_pos, cell_size)\n",
    "\n",
    "    # 添加鼠标点击退出功能\n",
    "    turtle.onscreenclick(lambda x, y: turtle.bye())\n",
    "\n",
    "    # 进入主循环\n",
    "    turtle.done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "560e263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_board(board_list[-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8772bf4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['O', 'O', 'X', 'O', 'O', 'O', 'O'],\n",
       "  ['O', 'O', '.', 'O', 'X', 'X', '.'],\n",
       "  ['.', '.', '.', 'X', 'X', 'O', '.'],\n",
       "  ['O', 'X', 'X', 'X', 'X', 'O', '.'],\n",
       "  ['O', 'X', 'X', 'X', 'X', '.', '.'],\n",
       "  ['.', '.', '.', '.', 'X', '.', '.'],\n",
       "  ['.', 'X', '.', '.', 'O', '.', '.']],\n",
       " [['O', 'O', 'X', 'O', 'X', '.', '.'],\n",
       "  ['O', 'O', '.', '.', 'X', 'X', '.'],\n",
       "  ['.', 'O', 'O', 'O', 'O', 'O', '.'],\n",
       "  ['.', 'X', 'O', 'X', 'X', 'X', '.'],\n",
       "  ['.', 'X', '.', '.', 'X', '.', 'O'],\n",
       "  ['.', '.', '.', '.', 'X', '.', '.'],\n",
       "  ['.', '.', '.', '.', '.', '.', 'X']],\n",
       " [['O', '.', '.', 'X', 'X', 'X', 'O'],\n",
       "  ['O', 'O', '.', 'X', 'O', 'O', 'O'],\n",
       "  ['.', '.', 'O', '.', 'X', 'X', '.'],\n",
       "  ['.', 'X', 'X', 'O', 'X', 'X', '.'],\n",
       "  ['X', '.', '.', 'X', 'X', 'X', '.'],\n",
       "  ['O', '.', 'O', '.', 'X', 'X', '.'],\n",
       "  ['O', 'X', 'O', 'O', 'O', 'O', 'O']],\n",
       " [['O', 'O', 'X', 'O', '.', '.', '.'],\n",
       "  ['.', '.', '.', 'O', '.', '.', '.'],\n",
       "  ['.', '.', '.', 'O', 'O', '.', '.'],\n",
       "  ['.', 'X', 'X', 'X', 'X', 'X', '.'],\n",
       "  ['.', '.', '.', '.', '.', '.', '.'],\n",
       "  ['.', '.', '.', '.', '.', '.', '.'],\n",
       "  ['.', 'X', '.', '.', '.', '.', '.']],\n",
       " [['.', '.', 'O', '.', 'X', '.', '.'],\n",
       "  ['.', '.', 'O', 'X', 'O', 'O', 'O'],\n",
       "  ['.', '.', 'O', 'O', 'X', 'X', 'O'],\n",
       "  ['X', 'X', 'X', 'O', 'X', '.', '.'],\n",
       "  ['X', '.', '.', 'O', 'X', 'O', '.'],\n",
       "  ['.', '.', 'X', 'O', 'X', '.', '.'],\n",
       "  ['.', 'X', '.', 'O', 'O', 'X', '.']],\n",
       " [['O', '.', 'O', '.', '.', '.', '.'],\n",
       "  ['.', '.', '.', '.', 'O', '.', '.'],\n",
       "  ['O', '.', '.', '.', 'O', 'X', '.'],\n",
       "  ['X', 'O', 'X', 'X', 'X', '.', '.'],\n",
       "  ['.', '.', '.', 'X', '.', '.', '.'],\n",
       "  ['.', 'O', 'X', '.', '.', '.', '.'],\n",
       "  ['.', 'X', '.', '.', '.', '.', '.']],\n",
       " [['O', '.', '.', 'O', 'O', '.', '.'],\n",
       "  ['.', '.', '.', '.', 'X', '.', '.'],\n",
       "  ['.', '.', '.', '.', 'X', '.', '.'],\n",
       "  ['.', '.', '.', '.', 'X', '.', '.'],\n",
       "  ['.', '.', '.', '.', 'X', '.', '.'],\n",
       "  ['.', '.', '.', '.', 'X', '.', '.'],\n",
       "  ['.', '.', '.', 'O', '.', '.', '.']],\n",
       " [['.', '.', 'O', 'O', 'X', '.', 'O'],\n",
       "  ['O', 'O', '.', '.', 'X', 'O', '.'],\n",
       "  ['.', 'O', '.', '.', 'O', 'X', '.'],\n",
       "  ['.', 'X', 'X', 'O', 'X', 'X', '.'],\n",
       "  ['.', 'X', 'O', '.', 'X', '.', '.'],\n",
       "  ['.', '.', '.', '.', 'X', '.', '.'],\n",
       "  ['.', '.', '.', '.', '.', '.', '.']],\n",
       " [['O', '.', '.', '.', 'O', '.', '.'],\n",
       "  ['.', '.', '.', '.', 'X', '.', '.'],\n",
       "  ['O', '.', '.', '.', 'X', '.', '.'],\n",
       "  ['.', '.', 'X', '.', 'X', '.', '.'],\n",
       "  ['.', '.', 'O', '.', 'X', '.', '.'],\n",
       "  ['.', '.', '.', '.', 'X', '.', '.'],\n",
       "  ['.', '.', '.', '.', '.', 'O', '.']],\n",
       " [['O', 'O', 'O', 'O', 'O', '.', 'O'],\n",
       "  ['.', '.', '.', 'X', 'X', 'O', 'O'],\n",
       "  ['O', 'O', '.', '.', 'X', 'X', 'X'],\n",
       "  ['.', 'O', 'X', '.', 'X', 'O', '.'],\n",
       "  ['.', 'O', '.', 'O', 'X', 'X', '.'],\n",
       "  ['.', '.', 'X', 'O', 'O', 'X', '.'],\n",
       "  ['X', 'X', 'X', '.', '.', 'X', 'X']]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c08d66e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlphaZeroNet(\n",
       "  (conv1): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (policy_conv): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (policy_fc): Linear(in_features=196, out_features=49, bias=True)\n",
       "  (value_conv): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (value_fc1): Linear(in_features=98, out_features=64, bias=True)\n",
       "  (value_fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6197f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
